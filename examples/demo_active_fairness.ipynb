{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T23:37:13.732412Z",
     "start_time": "2018-08-19T23:36:41.958772Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error: cannot import name 'Model'\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset, MexicoDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "from aif360.algorithms.inprocessing.active_fairness import ActiveFairness\n",
    "from aif360.algorithms.inprocessing.active_fairness import calibrate_probs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "\n",
    "privileged_groups = [{'young': 1}]\n",
    "unprivileged_groups = [{'young': 0}]\n",
    "\n",
    "\n",
    "dataset_orig = MexicoDataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.8], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56244, 186)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Testing Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14061, 186)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ftype', 'urban', 'young']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.]), array([1.])] [array([0.]), array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23', 'x24', 'x25', 'x26', 'x27', 'x28', 'x29', 'x30', 'x31', 'x32', 'x33', 'x34', 'x35', 'x36', 'x37', 'x38', 'x39', 'x40', 'x41', 'x42', 'x43', 'x44', 'x45', 'x46', 'x47', 'x48', 'x49', 'x50', 'x51', 'x52', 'x53', 'x54', 'x55', 'x56', 'x57', 'x58', 'x59', 'x60', 'x61', 'x62', 'x63', 'x64', 'x65', 'x66', 'x67', 'x68', 'x69', 'x70', 'x71', 'x72', 'x73', 'x74', 'x75', 'x76', 'x77', 'x78', 'x79', 'x80', 'x81', 'x82', 'x83', 'x84', 'x85', 'x86', 'x87', 'x88', 'x89', 'x90', 'x91', 'x92', 'x93', 'x94', 'x95', 'x96', 'x97', 'x98', 'x99', 'x100', 'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x107', 'x108', 'x109', 'x110', 'x111', 'x112', 'x113', 'x114', 'x115', 'x116', 'x117', 'x118', 'x119', 'x120', 'x121', 'x122', 'x123', 'x124', 'x125', 'x126', 'x127', 'x128', 'x129', 'x130', 'x131', 'x132', 'x133', 'x134', 'x135', 'x136', 'x137', 'x138', 'x139', 'x140', 'x141', 'x142', 'x143', 'x144', 'x145', 'x146', 'x147', 'x148', 'x149', 'x150', 'x151', 'x152', 'x153', 'x154', 'x155', 'x156', 'x157', 'x158', 'x159', 'x160', 'x161', 'x162', 'x163', 'x164', 'x165', 'x166', 'x167', 'x168', 'x169', 'x170', 'x171', 'x172', 'x173', 'x174', 'x175', 'x176', 'x177', 'x178', 'x179', 'x180', 'x181', 'x182', 'x183', 'ftype', 'urban', 'young']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Testing Dataset shape\"))\n",
    "print(dataset_orig_test.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.095692\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.112021\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Scaled dataset - Verify that the scaling does not affect the group label statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.095692\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.112021\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn plan classifier without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest model \n",
    "n_trees = 16\n",
    "max_leaf_nodes = 350\n",
    "'''\n",
    "We use tranining data to train the RF, and apply Active Fairness for both training dataset (Generate df_p_train,df_qa_train,df_y_train,df_mc_train,df_X_train)\n",
    "and testing dataset  (Generate df_p_test,df_qa_test,df_y_test,df_mc_test,df_X_test) \n",
    "'''\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, n_estimators=n_trees, random_state=0)\n",
    "\n",
    "AF = ActiveFairness(dataset_orig_train, dataset_orig_test,rf,\n",
    "                          privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          sensitive_features = ['ftype', 'urban', 'young'],\n",
    "                          target_label = ['Outcome']\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_noactivefair_train = AF.predict(True)\n",
    "dataset_noactivefair_test = AF.predict(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without active fairness - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.166879\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.165569\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without active fairness - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.780741\n",
      "Test set: Balanced classification accuracy = 0.740737\n",
      "Test set: Disparate impact = 0.551996\n",
      "Test set: Equal opportunity difference = -0.177510\n",
      "Test set: Average odds difference = -0.128265\n",
      "Test set: Theil_index = 0.181856\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without active fairness - dataset metrics\"))\n",
    "metric_dataset_noactivefair_train = BinaryLabelDatasetMetric(dataset_noactivefair_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_noactivefair_train.mean_difference())\n",
    "\n",
    "metric_dataset_noactivefair_test = BinaryLabelDatasetMetric(dataset_noactivefair_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_noactivefair_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without active fairness - classification metrics\"))\n",
    "classified_metric_noactivefair_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_noactivefair_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_noactivefair_test.accuracy())\n",
    "TPR = classified_metric_noactivefair_test.true_positive_rate()\n",
    "TNR = classified_metric_noactivefair_test.true_negative_rate()\n",
    "bal_acc_noactivefair_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_noactivefair_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_noactivefair_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_noactivefair_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_noactivefair_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_noactivefair_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AF algorithm to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 0 END 500\n",
      "START 500 END 1000\n",
      "START 1000 END 1500\n",
      "START 1500 END 2000\n",
      "START 2000 END 2500\n",
      "START 2500 END 3000\n",
      "START 3000 END 3500\n",
      "START 3500 END 4000\n",
      "START 4000 END 4500\n",
      "START 4500 END 5000\n",
      "START 5000 END 5500\n",
      "START 5500 END 6000\n",
      "START 6000 END 6500\n",
      "START 6500 END 7000\n",
      "START 7000 END 7500\n",
      "START 7500 END 8000\n",
      "START 8000 END 8500\n",
      "START 8500 END 9000\n",
      "START 9000 END 9500\n",
      "START 9500 END 10000\n",
      "START 10000 END 10500\n",
      "START 10500 END 11000\n",
      "START 11000 END 11500\n",
      "START 11500 END 12000\n",
      "START 12000 END 12500\n",
      "START 12500 END 13000\n",
      "START 13000 END 13500\n",
      "START 13500 END 14000\n",
      "START 14000 END 14061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' update '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_method = 'feat-imp'\n",
    "privilige_feature = 'young'\n",
    "privilige_value = 1\n",
    "unprivilige_value = 0\n",
    "privilige_num = 10\n",
    "unprivilige_num =  10\n",
    "df_p_test,df_qa_test,df_y_test,df_mc_test,df_X_test = AF.run_algo_in_parallel(\n",
    "                                                                        feat_method,\n",
    "                                                                        privilige_feature,\n",
    "                                                                        privilige_variable_value = privilige_value,\n",
    "                                                                        unprivilige_variable_value = unprivilige_value,\n",
    "                                                                        pri_num_feature_fetched = privilige_num,\n",
    "                                                                        un_pri_num_feature_fetched = unprivilige_num,\n",
    "                                                                        verbose=0,\n",
    "                                                                        nr_of_batches=40,\n",
    "                                                                        batch_size=500,\n",
    "                                                                        n_jobs=2,\n",
    "                                                                        save_to_file=False) # Run this code in testing dataset\n",
    "''' updated above'''\n",
    "\n",
    "\n",
    "# This is used to run the original code\n",
    "# df_p_test,df_qa_test,df_y_test,df_mc_test,df_X_test = AF.run_algo_in_parallel(\n",
    "#                                                                         feat_method,\n",
    "#                                                                         verbose=0,\n",
    "#                                                                         nr_of_batches=40,\n",
    "#                                                                         batch_size=500,\n",
    "#                                                                         n_jobs=2,\n",
    "#                                                                         save_to_file=False,\n",
    "#                                                                         static_threshold = 11) # Run this code in testing dataset\n",
    "\n",
    "# df_p_train,df_qa_train,df_y_train,df_mc_train,df_X_train = AF.run_algo_in_parallel(\n",
    "#                                                                         feat_method,\n",
    "#                                                                         verbose=0,\n",
    "#                                                                         nr_of_batches=120,\n",
    "#                                                                         batch_size=500,\n",
    "#                                                                         n_jobs=4,\n",
    "#                                                                         save_to_file=False, \n",
    "#                                                                         run_on_training=True)  # Run this code in training dataset   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 0.5\n",
    "dataset_noactive_train = deepcopy(AF.dataset_train)\n",
    "dataset_noactive_test = deepcopy(AF.dataset_test)\n",
    "dataset_active_test = deepcopy(AF.dataset_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-8ee48757e18f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-8ee48757e18f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    update the label\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(AF.X_te_sensitiveAtarget)):\n",
    "    if AF.X_te_sensitiveAtarget.iloc[i, AF.feature2columnmap[privilige_feature]] == privilige_value:\n",
    "        dataset_active_test.labels[i] = 0 if df_p_test.iloc[i, privilige_num - 1] < threshold else 1\n",
    "    else:\n",
    "        dataset_active_test.labels[i] = 0 if df_p_test.iloc[i, unprivilige_num - 1] < threshold else 1\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the dataset from plain model (without noactivefair)\n",
    "display(Markdown(\"#### Plain model - without active fairness - dataset metrics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_noactivefair_train.mean_difference())\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_noactivefair_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with active fairness - dataset metrics\"))\n",
    "\n",
    "# metric_dataset_noactivefair_train = BinaryLabelDatasetMetric(dataset_noactive_train, \n",
    "#                                              unprivileged_groups=unprivileged_groups,\n",
    "#                                              privileged_groups=privileged_groups)\n",
    "\n",
    "# print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_noactivefair_train.mean_difference())\n",
    "\n",
    "metric_dataset_activefair_test = BinaryLabelDatasetMetric(dataset_active_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_activefair_test.mean_difference())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without active fairness - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_noactivefair_test.accuracy())\n",
    "TPR = classified_metric_noactivefair_test.true_positive_rate()\n",
    "TNR = classified_metric_noactivefair_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_noactivefair_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_noactivefair_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_noactivefair_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_noactivefair_test.theil_index())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with active fairness - classification metrics\"))\n",
    "classified_metric_active_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_active_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_active_test.accuracy())\n",
    "TPR = classified_metric_active_test.true_positive_rate()\n",
    "TNR = classified_metric_active_test.true_negative_rate()\n",
    "bal_acc_active_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_active_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_active_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_active_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_active_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_active_test.theil_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
